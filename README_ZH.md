# Gateway Bottleneck Lab

[English](./README.md) | [Deutsch](./README_DE.md)

> 基于一个经过脱敏与简化的真实生产问题。
> 本项目展示了为什么小粒度、聚焦的性能测试——特别是组件级和集成级的测试——能够帮助在早期发现隐藏的性能瓶颈，
> 从而避免在大型系统中产生复杂且高成本的问题。

## 🎯 项目目标

本仓库提供了一个**最小化**、**可复现**的测试环境，用于演示在典型的分布式架构
（Gateway → Authentication → Business Logic）中常见的性能瓶颈。

整个场景基于一个**真实**出现过的**生产问题**，但已经经过完全的**清理**、**脱敏**与**简化**，**不包含任何敏感信息**。

本项目的目的是帮助开发者理解：
- 为什么细粒度性能测试（从简单 **Benchmark** 到**组件级**、**集成级**性能测试）与系统级别的性能测试同样重要；
- 如何构建**可重复**、**可控**的性能测试环境，从而在**早期**发现潜在的性能问题。
- 
## 🧱 项目概览

本项目模拟了一条简化但接近真实的服务调用链，包括：

- **gateway-for-test** —— 网关
- **auth-service-for-test** —— 鉴权服务
- **service-for-test** —— 业务服务

所有组件均被完全**容器化**，**可独立运行**，也可以**通过 Docker 一键启动**整个环境。

在包含完整示例代码的分支中（例如 **0.0.1**、**0.0.2** 等），
借助**resources/** 目录下包含的 `docker-compose.yml`，可以一键启动完整的测试环境。

**main** 分支仅包含项目说明文档，不包含可运行代码。

架构示意图：

![Architektur](https://raw.githubusercontent.com/ksewen/Bilder/main/20251116160740231.png)

### 分支说明

各个分支代表系统的不同阶段与不同问题的复现状态：
- 分支 [**0.0.1**](https://github.com/ksewen/performance-test-example/tree/0.0.1)：包含一个刻意构造的性能瓶颈。
分析显示问题来源于：**在 WebFlux 中使用了基于 RestTemplate 的同步阻塞 HttpClient**，在高并发下会产生明显延迟。
- 分支 [**0.0.2**](https://github.com/ksewen/performance-test-example/tree/0.0.2)：修复了 [**0.0.1**](https://github.com/ksewen/performance-test-example/tree/0.0.1) 中的性能瓶颈，并对优化结果进行了评估。
在评估过程中进一步发现：**Logback 在输出日志时产生了可观察到的性能损耗**。
- 分支 [**0.0.3**](https://github.com/ksewen/performance-test-example/tree/0.0.3)：修复并分析了 [**0.0.2**](https://github.com/ksewen/performance-test-example/tree/0.0.2) 中发现的问题。
此外还包含了一个针对 **UUID 生成过程中随机数阻塞**问题的解决方案与详细说明。

每个分支都包含对应的测试方法、详细分析与测量结果。

## 🧭 性能分析的核心认识

在这个项目的过程中可以清楚地看到：性能测试的目的并**不是**为了找到**某一个“正确的数字”**。  
真实系统的性能会受到许多因素影响——例如 CPU 性能、网络状况、容器环境、软件版本等，因此结果经常会发生变化。

基于这些经验，可以总结出三条在任何性能测试或瓶颈分析中都非常重要的原则：

### 组件级或集成级测试并不能保证整个系统的完美性能

即使一个服务或组件在集成测试中表现非常好，也不代表它放入完整系统后仍能保持同样的性能。

但细粒度测试非常有价值，因为它们能够：

- **提前发现明显错误**，避免问题在大型系统中变得昂贵且难以复现
- **节省时间和成本**，因为在系统复杂化之前就能暴露问题
- 为开发人员——尤其是编写**通用或可复用组件**的开发人员——提供检查质量和稳定性的有效工具

这些测试不能取代系统级测试，但它们是任何严谨性能分析的基础。

### 性能始终依赖于具体环境

不同环境下的测试结果可能差异非常大。 
即使是微小的变化——硬件、线程池配置、网络延迟——都可能导致完全不同的表现。

因此，性能测试必须在**多次**、**真实且接近生产的条件下**进行，结果才具有意义。

### 性能优化永远是一项成本与收益的权衡

在很多实际场景中，应该把**继续优化代码**与**直接扩容节点**的成本和收益进行比较。

全面的性能优化往往成本很高，例如：

- 搭建 1:1 或比例缩小的**测试环境**
- 构造或导入 **真实或具有代表性的测试数据**
- 进行 **多轮测试**
- 花费大量 **时间与人力成本**

因此需要特别注意：

👉 **性能目标应该是“足够好”，而不是“做到极限”。** 
只有当优化带来的收益明显高于成本时，优化才真正有意义。

本项目展示的正是这种**方法论**：  
目标不是追求某个具体数字，而是学习如何系统地发现、分析和解决性能瓶颈，并将这种思维方式应用到真实生产系统中。


## 🐳 使用 Docker 运行

### 构建 Docker 镜像

在项目根目录下，使用提供的脚本构建各服务镜像：

```shell
gateway-for-test/resources/scripts/build-image.sh -d .
service-for-test/resources/scripts/build-image.sh -d .
auth-service-for-test/resources/scripts/build-image.sh -d .
```

### 启动完整环境

```shell
cd resources && \
docker-compose --compatibility -f docker-compose.yml up
```

## 🧪 功能验证

Gateway 提供了一个简单的测试路由，可用于基础功能验证：

```shell
curl http://127.0.0.1:38071/service/hello
```
如需进一步测试（例如并发访问、模拟高压场景），可以使用外部工具模拟请求。
我个人选择了轻量级工具 [**wrk**](https://github.com/wg/wrk)￼，因为它简单、易复现，适用于本项目。

更完整的示例、测试脚本以及性能对比结果，请参考对应的分支（如 **0.0.1**、**0.0.2**）。