# Die dritte Testrunde

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_ZH.md)

## ğŸ¯ Ziel

Nach den Optimierungen aus der **zweiten Testrunde** sollte erneut Ã¼berprÃ¼ft werden, wie sich das Gesamtsystem Ã¼ber das
Gateway verhÃ¤lt.
Die Ziele dieser Runde waren:

- die **Auswirkung des AsyncAppender** realistisch zu bewerten
- **weitere mÃ¶gliche** versteckte EngpÃ¤sse aufzudecken
- Unterschiede zwischen diesem Labor-Setup und realen Produktionsumgebungen besser zu verstehen
- zu prÃ¼fen, ob das System nach den bisherigen Anpassungen nÃ¤her an die erwartete Systemleistung herankommt

## Durchsatztest nach der Behebung und unerwartetes Ergebnis

Nach der Aktivierung des **AsyncAppender** wurde erneut ein fÃ¼nfminÃ¼tiger Durchsatztest ausgefÃ¼hrt:

**Ergebnis**:

> Requests/sec:   3919.34

Originales Protokoll:

```shell
Running 5m test @ http://127.0.0.1:38071/service/hello
  5 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.56ms  545.34us  29.72ms   92.53%
    Req/Sec   787.67     33.81     0.89k    77.66%
  Latency Distribution
     50%    2.48ms
     75%    2.69ms
     90%    2.94ms
     99%    4.09ms
  1176033 requests in 5.00m, 143.56MB read
Requests/sec:   3919.34
Transfer/sec:    489.92KB
```

Der Durchsatz lag damit ca. 4 % **unter** dem Wert der zweiten Testrunde.

### Interpretation

Obwohl der AsyncAppender theoretisch den Request-Thread entlasten sollte, hat er in diesem Setup zu einer **geringfÃ¼gig
schlechteren** Performance gefÃ¼hrt.
Eine plausible ErklÃ¤rung liegt in den beschrÃ¤nkten CPU-Ressourcen (nur **zwei Cores** in diesem Labor-Setup), wodurch
der
zusÃ¤tzliche Logger-Hintergrundthread mehr Kontextwechsel verursacht als Nutzen bringt.

In Produktionssystemen mit **mehr CPU-Kernen** wÃ¤re das Ergebnis wahrscheinlich anders.

## Vertiefte Analyse

### Reduzieren der Log-Ausgabe

Das Log-Level wurde auf **ERROR** reduziert, um mÃ¶glichst wenig Einfluss durch I/O im Request-Pfad zu erzeugen.

Dies ist auch im realen Produktionsbetrieb eine Ã¼bliche und praktikable MaÃŸnahme, da moderne Microservice-Plattformen
**dynamische Log-Level-Ã„nderungen** unterstÃ¼tzen.

Nach der Reduktion des Log-Levels auf ERROR ergab der erneute Test.

**Ergebnis**:

> Requests/sec:   4369.11

Originales Protokoll:

```shell
Running 5m test @ http://127.0.0.1:38071/service/hello
  5 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.31ms  761.12us  69.92ms   96.95%
    Req/Sec     0.88k    59.38     0.96k    85.50%
  Latency Distribution
     50%    2.21ms
     75%    2.36ms
     90%    2.56ms
     99%    4.36ms
  1311067 requests in 5.00m, 160.04MB read
Requests/sec:   4369.11
Transfer/sec:    546.14KB
```

Der Durchsatz **stieg** damit um etwa **6 %**.
BerÃ¼cksichtigt man Messungenauigkeiten, deckt sich diese Verbesserung mit der in der
[letzten Runde](https://github.com/ksewen/gateway-bottleneck-lab/tree/0.0.2) beobachteten
CPU-Zeit-Belastung durch Logging.

Damit wurde bestÃ¤tigt:

Die Reduktion der Log-Ausgabe wirkt sich direkt **positiv** auf den Durchsatz aus.

### Analyse nach Log-Optimierung

Um sicherzustellen, dass das Gateway keine weiteren strukturellen EngpÃ¤sse aufweist, wurden CPU- und Thread-Profile
erneut Ã¼berprÃ¼ft:

![cpu-views-call-tree](https://github.com/ksewen/Bilder/blob/main/202308190029673.png?raw=true "CPU Views - Call Tree")

Beobachtungen:

- keine auffÃ¤llig hÃ¤ufigen **BLOCK-** oder **WAIT-ZustÃ¤nde**
- keine Hinweise auf **Event-Loop-Blockierungen**

Damit war klar:
Das Gateway selbst wies nach der Log-Optimierung **keine offensichtlichen Bottlenecks** mehr auf.

## Weitere ErklÃ¤rungen

### Verbesserung der Profiling-Strategie â€“ Entdeckung eines neuen Engpasses

In den vorherigen Testrunden war der JProfiler-Paketfilter bewusst auf
`com.github.ksewen` eingeschrÃ¤nkt worden, um Zeit und Ressourcen zu sparen.

FÃ¼r eine umfassendere Analyse wurde dieser Filter nun entfernt, damit der gesamte
Request-Flow â€” inklusive aller Bibliotheken, Framework-Komponenten und JVM-internen AblÃ¤ufe â€”
sichtbar wird. ZusÃ¤tzlich wurden Monitors & Locks aktiviert, um mÃ¶gliche Blockierungen besser zu erkennen.

Erst durch diese erweiterte Sicht wurde ein zuvor verborgener Engpass sichtbar:

ğŸ‘‰ die Blockierung bei der **UUID-Generierung**.

### Engpass bei der UUID-Generierung

Im Filter
[TracingGlobalFilter](https://github.com/ksewen/gateway-bottleneck-lab/blob/0.0.3/gateway-for-test/src/main/java/com/github/ksewen/performance/test/gateway/filter/tracing/TracingGlobalFilter.java)
wird fÃ¼r jede Anfrage eine neue UUID erzeugt: `UUID.randomUUID()`

Unter Linux verwendet diese Methode standardmÃ¤ÃŸig `SecureRandom`.
Wenn `SecureRandom` auf eine blockierende Quelle wie /dev/random zugreift und die Entropie knapp ist, kÃ¶nnen
Wartezeiten entstehen.

Dies wurde im Profiling sichtbar:

![Blockierung-1](https://raw.githubusercontent.com/ksewen/Bilder/main/202308201438817.png)

![Blockierung-2](https://raw.githubusercontent.com/ksewen/Bilder/main/202308201439720.png)

![Statistik der Blockierung](https://raw.githubusercontent.com/ksewen/Bilder/main/202308201439000.png)

### Behebung in diesem Labor

Um die Blockierung zu vermeiden, wurde die JVM mit folgender Option gestartet:

```shell
-Djava.security.egd=file:/dev/urandom
```

Damit nutzt `SecureRandom` eine **nichtblockierende Entropiequelle**, wodurch die UUID-Generierung stabiler wird.

### Zusammenhang mit Java-Versionen (*JEP 273*)

Dieses Verhalten hÃ¤ngt auch mit der **Java-Version** zusammen:

- Produktionssystem damals: **Java 8**
- Dieses Labor: **Java 17**

Mit **Java 9** wurde `SecureRandom` gemÃ¤ÃŸ *JEP 273* grundlegend Ã¼berarbeitet:

ğŸ“Œ https://openjdk.org/jeps/273

Dadurch unterscheidet sich das Verhalten der UUID-Generierung zwischen **Java 8** und **Java 17** deutlich.

> Eine vertiefte Analyse des Problems unter Java 8 sowie eine konkrete LÃ¶sung finden Sie in meinem Projekt:  
> ğŸ‘‰ [**uuid-benchmark**](https://github.com/ksewen/uuid-benchmark)

### Erwartetes Ergebnis

Nach der Anpassung wurde erwartet:

- stabile UUID-Generierung **ohne Blockierung**
- **geringere Latenzen** bei hoher Last
- **leichter Anstieg** des Durchsatzes

> ErklÃ¤rung:
> Die endgÃ¼ltige Validierung bleibt jedoch **aufgrund begrenzter Ressourcen** in diesem Labor-Setup eingeschrÃ¤nkt.

## ğŸŸ© Gesamtfazit

- Der AsyncAppender zeigte in dieser Runde **keine Verbesserung** und fÃ¼hrte zu einer **Leistungsverschlechterung von 4 %**.
- Durch das Reduzieren des Log-Levels konnte der Durchsatz auf **6 % erhÃ¶ht** werden.
- Das Gateway selbst weist nach den Anpassungen **keine klaren EngpÃ¤sse** mehr auf.
- Durch eine verfeinerte Profilierung wurde die **UUID-Generierung** als versteckter Engpass identifiziert.
- Die LÃ¶sung mittels `egd=/dev/urandom` ist im **Java-17-Kontext** wirksam, **unterscheidet** sich aber vom
  **Java-8-Verhalten** (*JEP 273*).
- Diese Runde zeigt eindeutig, wie wichtig Profiling-Strategien, Log-Management, JVM-Wissen und Umgebungsfaktoren fÃ¼r
  Performanceanalysen sind.
- Und erneut bestÃ¤tigt sich:
  Performance ist **kontextabhÃ¤ngig**, **methodisch** und **selten perfekt reproduzierbar**.