# Gateway Bottleneck Lab

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README_ZH.md)

> Basierend auf einem realen Produktionsproblem (vereinfacht). Dieses Projekt zeigt, warum kleine, gezielte
> Performance-Tests â€“ insbesondere auf Komponenten- oder Integrationsebene â€“ helfen, versteckte EngpÃ¤sse frÃ¼h zu
> erkennen
> und kostspielige Probleme in groÃŸen Systemen zu vermeiden.

## ğŸ¯ Ziel des Projekts

Dieses Repository stellt eine **minimal reproduzierbare Umgebung** bereit, die typische LeistungsengpÃ¤sse in einer
verteilten Architektur (Gateway â†’ Authentication â†’ Business Logic) demonstriert.

Der Fall basiert auf einem **realen Produktionsproblem**, wurde jedoch vollstÃ¤ndig **bereinigt**, **vereinfacht** und
**ohne vertrauliche Inhalte** nachgebaut.

Das Projekt dient als technische Referenz fÃ¼r Entwicklerinnen und Entwickler, um folgende Aspekte besser zu verstehen:

- warum feingranulare Performance-Tests - von **Benchmarking** von einzelner Komponente bis hin zu **Integration** mit
  verschiedenen Komponenten genauso wichtig sind wie **systemweiten Performance-Tests**.
- wie sich **reproduzierbare Testumgebungen** aufbauen lassen, um Leistungsprobleme **frÃ¼hzeitig** zu entdecken.

## ğŸ§± ProjektÃ¼berblick

Dieses Projekt bildet eine **vereinfachte**, aber **realistische** Servicekette ab, bestehend aus:

- **gateway-for-test** â€“ API Gateway
- **auth-service-for-test** â€“ Authentifizierungsservice
- **service-for-test** â€“ Business Service

Alle Komponenten sind vollstÃ¤ndig **containerisiert** und kÃ¶nnen **unabhÃ¤ngig voneinander** oder gemeinsam **Ã¼ber Docker
ausgefÃ¼hrt** werden.

In den Branches, die den vollstÃ¤ndigen Beispielcode enthalten (z.B. **0.0.1**, **0.0.2** usw.), befindet sich im
Verzeichnis **resources/** eine `docker-compose.yml`, mit der die gesamte Testumgebung schnell gestartet werden kann.

Der **main**-Branch enthÃ¤lt ausschlieÃŸlich die Projektbeschreibung und keine lauffÃ¤higen Artefakte.

Eine einfache Darstellung der Architektur:

![Architektur](https://raw.githubusercontent.com/ksewen/Bilder/main/20251116160740231.png)

### Branches

Die verschiedenen Branches enthalten unterschiedliche ZustÃ¤nde des Systems:

- Der Branch [**0.0.1**](https://github.com/ksewen/performance-test-example/tree/0.0.1) enthÃ¤lt bewusst einen
  reproduzierbaren Leistungsengpass. Die Analyse zeigt, dass die Ursache im Einsatz eines **RestTemplate mit synchronem
  HttpClient innerhalb WebFlux** liegt, was unter Last zu deutlichen Latenzen fÃ¼hrt.
- Der Branch [**0.0.2**](https://github.com/ksewen/performance-test-example/tree/0.0.2) behebt den Engpass aus dem
  Branch [**0.0.1**](https://github.com/ksewen/performance-test-example/tree/0.0.1) und enthÃ¤lt eine Bewertung der
  Optimierungsergebnisse. Dabei wurde zusÃ¤tzlich festgestellt, dass **Logback beim Schreiben von Logs spÃ¼rbare
  Performance-Kosten verursacht**.
- Der Branch [**0.0.3**](https://github.com/ksewen/performance-test-example/tree/0.0.3) korrigiert und analysiert die im
  Branch [**0.0.2**](https://github.com/ksewen/performance-test-example/tree/0.0.2) erkannten Probleme. Er enthÃ¤lt zudem
  eine LÃ¶sung sowie detaillierte Informationen zur **Blockierung bei der UUID-Generierung**.

Die jeweiligen Branches enthalten ausfÃ¼hrliche ErlÃ¤uterungen zu Testmethoden und Messergebnissen.

## ğŸ§­ Zentrale Erkenntnisse zur Performanceanalyse

WÃ¤hrend der Arbeit an diesem Projekt hat sich klar gezeigt, dass Performanceanalysen **nicht** darauf abzielen, **ein einzig
â€richtigesâ€œ Ergebnis zu reproduzieren**. In realen Systemen hÃ¤ngt das Verhalten stark von der jeweiligen Umgebung ab â€“
etwa CPU-Leistung, Netzwerktopologie, Container-Runtime oder verwendeten Softwareversionen.

Daraus ergeben sich drei zentrale Prinzipien, die in jedem Performance- oder Bottleneck-Lab berÃ¼cksichtigt werden
sollten:

### Komponenten- oder Integrationstests garantieren keine perfekte Systemperformance

Selbst wenn ein einzelner Dienst oder eine Komponente im Benchmark optimal lÃ¤uft, bedeutet das nicht, dass sich
dieselbe Performance in einem vollstÃ¤ndigen, realen System erzielen lÃ¤sst.

Der Nutzen dieser feingranularen Tests liegt vor allem darin:

- **frÃ¼hzeitig offensichtliche Fehler zu entdecken**, bevor sie in komplexen Systemen teuer und schwer reproduzierbar
  werden
- **Kosten zu sparen**, weil Probleme bereits auf Komponenten- oder Integrationsebene sichtbar werden
- Entwicklerinnen und Entwicklern â€“ insbesondere jenen, die **allgemeine oder wiederverwendbare Komponenten**
  schreiben â€“
  ein zuverlÃ¤ssiges Werkzeug fÃ¼r QualitÃ¤ts- und Robustheitskontrolle an die Hand zu geben

Diese Tests ersetzen nicht die Systemtests, aber sie bilden die Grundlage fÃ¼r jede solide Performanceanalyse.

### Performance ist immer kontextabhÃ¤ngig

Testergebnisse unterscheiden sich je nach Umgebung oft deutlich.
Selbst kleine Ã„nderungen â€“ andere Hardware, angepasste Thread-Pools, verÃ¤nderte Netzwerklatenzen â€“ kÃ¶nnten das Verhalten
spÃ¼rbar beeinflussen.

Performanceanalysen mÃ¼ssen daher **mehrmals** und **unter realistischen Bedingungen** durchgefÃ¼hrt werden, um aussagekrÃ¤ftig zu
sein.

### Optimierung ist immer eine Kosten-Nutzen-AbwÃ¤gung

In vielen Anwendungsszenarien ist es ebenso wichtig, den Nutzen einer Optimierung mit den Kosten einer einfachen
horizontalen Skalierung oder Knoten-Erweiterung zu vergleichen.
Systemweite Performanceoptimierung kann sehr aufwendig werden:
- Aufbau einer 1:1 oder proportional skalierten **Testumgebung**
- Erstellen oder Importieren von reprÃ¤sentativen **Testdaten**
- DurchfÃ¼hrung **mehrerer Testzyklen**
- hoher **Zeit-** und **Personalaufwand** wÃ¤hrend der Analyse

Dabei sollte stets bedacht werden:

ğŸ‘‰ **Ziel ist â€ausreichendeâ€œ Performance, nicht maximale Performance.** 
Eine Optimierung ist nur dann sinnvoll, wenn ihr tatsÃ¤chlicher Nutzen den technischen und betrieblichen Aufwand rechtfertigt.

Dieses Projekt zeigt genau diese **Methodik**:  
Nicht das Erreichen einer bestimmten Zahl ist das Ziel, sondern das systematische Identifizieren, Analysieren und
Beheben von EngpÃ¤ssen â€“ und das realistische Ãœbertragen dieser Vorgehensweise auf produktive Systeme.

## ğŸ³ AusfÃ¼hrung mit Docker

### Docker-Images erstellen

Im Hauptverzeichnis kÃ¶nnen die Images aller Services durch die bereitgestellten Skripte gebaut werden:

```shell
gateway-for-test/resources/scripts/build-image.sh -d .
service-for-test/resources/scripts/build-image.sh -d .
auth-service-for-test/resources/scripts/build-image.sh -d .
```

### Umgebung starten

```shell
cd resources && \
docker-compose --compatibility -f docker-compose.yml up
```

## ğŸ§ª FunktionsprÃ¼fung

Das Gateway stellt eine einfache Test-Route bereit, die zur FunktionsprÃ¼fung aufgerufen werden kann:

```shell
curl http://127.0.0.1:38071/service/hello
```

FÃ¼r weitere Tests kann die Anwendung selbstverstÃ¤ndlich auch mit externen Tools simuliert werden, um parallele Zugriffe
oder hÃ¶here Last zu erzeugen.
Ich nutze dafÃ¼r ein leichtgewichtiges Tool wie [**wrk**](https://github.com/wg/wrk), da es sich gut fÃ¼r einfache und
reproduzierbare Lastszenarien eignet.

AusfÃ¼hrliche Beispiele und Messergebnisse befinden sich in den jeweiligen Branches (z.B. **0.0.1**, **0.0.2**).
